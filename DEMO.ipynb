{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "565e2650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import data process tools\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "# import neural network tools\n",
    "# tensorflow version: 2.10.1\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "# import mathematic tools\n",
    "import numpy as np\n",
    "# import illustration tools\n",
    "import matplotlib.pyplot as plt\n",
    "# import other python modules\n",
    "import random\n",
    "import os\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4f532ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial settings\n",
    "seed = 100\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73992b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def convolution_predict(token, model, window, mode=0):\n",
    "    # mode 0: return subtext that has closest score to text for binary label\n",
    "    # mode 1: return subtext that has highest score in same direction with text for binary label\n",
    "    # mode 2: return subtext that has closest score to text for multi label\n",
    "    # mode 3: return subtext that has highest score in same direction with text for multi label\n",
    "    text = ' '.join(token)\n",
    "    token_windows = [token[i:i+window] for i in range(len(token)-window+1)]\n",
    "    text_windows = list(map(lambda x: ' '.join(x), token_windows))\n",
    "    prediction = model.predict([text])[0]\n",
    "    window_prediction = model.predict(text_windows)\n",
    "    if mode == 0:\n",
    "        result = np.argmin(np.sum(np.absolute(window_prediction - prediction), axis=1))\n",
    "    elif mode == 1:\n",
    "        result = np.argmax(np.sum(window_prediction * prediction, axis=1))\n",
    "    elif mode == 2:\n",
    "        result = np.argmin(np.sum(np.absolute(window_prediction - prediction), axis=1))\n",
    "    elif mode == 3:\n",
    "        result = np.argmax(window_prediction[:, np.argmax(prediction)])\n",
    "    # get window of subtext of best similarity to original text\n",
    "    cause = text_windows[result]\n",
    "    return cause, (np.argmax(prediction) - 3)\n",
    "\n",
    "def get_formality(label):\n",
    "    if label > 0:\n",
    "        return 'formal'\n",
    "    elif label < 0:\n",
    "        return 'informal'\n",
    "    else:\n",
    "        return 'natural'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c0ed6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "example_data = pd.read_excel('tokenized_answers.xlsx')[['text', 'token', 'score']]\n",
    "example_data['token'] = example_data['token'].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dbad8c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x21c93f19de0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model\n",
    "model = tf.keras.models.load_model(os.getcwd() + \"/model/structure\")\n",
    "model.load_weights(os.getcwd() + \"/model/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3157e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "[[8.9285839e-01 3.9258625e-02 6.1685503e-02 4.3453872e-03 1.7473558e-03\n",
      "  9.8729790e-05 6.0565035e-06]]\n",
      "The formality predicted for text \"3 ) You ' re complaining about your stupid mistake\" is: -3\n",
      "The actual score is: -2.6\n"
     ]
    }
   ],
   "source": [
    "# predict formality for a piece of text\n",
    "sample_index = random.sample(range(len(example_data['text'])), 1)[0]\n",
    "sample_text = example_data['text'][sample_index]\n",
    "sample_token = example_data['token'][sample_index]\n",
    "sample_score = example_data['score'][sample_index]\n",
    "prediction = np.argmax(model.predict([sample_text])) - 3\n",
    "print(model.predict([sample_text]))\n",
    "print('The formality predicted for text \"' + sample_text + '\" is: ' + str(prediction))\n",
    "print('The actual score is: ' + str(sample_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d3d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "The text \"3 ) You ' re complaining about your stupid mistake\" is informal as it contains: \n",
      ") You ' re complaining\n"
     ]
    }
   ],
   "source": [
    "interpret = convolution_predict(sample_token, model, max(int(len(sample_token)/2),1), mode=3)\n",
    "print('''The text \"''' + sample_text + '''\" is ''' + get_formality(int(interpret[1])) + ' as it contains: ')\n",
    "print(interpret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3fe5c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trying(user_input=None):\n",
    "    if user_input == None:\n",
    "        user_input = input('Please input a piece of text, to check formality: ')\n",
    "    tokens = word_tokenize(user_input)\n",
    "    interpret = convolution_predict(tokens, model, max(int(len(tokens)/2),1), mode=3)\n",
    "    print('''The text \"''' + user_input + '''\" is ''' + get_formality(int(interpret[1])) + ' (' + str(int(interpret[1])) + ') as it contains: ')\n",
    "    print(interpret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd2de1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "The text \"yeah no big deal w u\" is informal (-2) as it contains: \n",
      "big deal w\n"
     ]
    }
   ],
   "source": [
    "# good performance to classify informal\n",
    "trying(\"yeah no big deal w u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e6ec66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "The text \"It is not a big deal.\" is natural (0) as it contains: \n",
      "It is not\n"
     ]
    }
   ],
   "source": [
    "# good performance to classify natural\n",
    "trying(\"It is not a big deal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14d759d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "The text \"From previous lemma in section 3.1, the authors can come to the conclusion that such a founding should not be a worrying sign. \" is natural (0) as it contains: \n",
      "to the conclusion that such a founding should not be a worrying\n"
     ]
    }
   ],
   "source": [
    "# not good performance to classify formal\n",
    "trying(\"From previous lemma in section 3.1, the authors can come to the conclusion that such a founding should not be a worrying sign. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a409af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a piece of text, to check formality: test\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "The text \"test\" is informal (-3) as it contains: \n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# It's your turn\n",
    "trying()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eac479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
